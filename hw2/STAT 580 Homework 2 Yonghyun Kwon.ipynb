{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Z = \\sqrt{Y}\\frac{X}{||X||}, \\ U = X^TX$ then apply change of variable formula to obtain the joint distribution of $Z$ and $U$.$X$ and $Y$ then can be written as\n",
    "\n",
    "\\begin{gather*}\n",
    "X = \\sqrt{U}\\frac{Z}{Z^TZ} \\\\\n",
    "Y = Z^TZ\n",
    "\\end{gather*}\n",
    "\n",
    "In this problem, however, direct derivation of Jacobian matrix is not easy. Therefore, substitute first and apply chain rule to obtain the determinant of Jacobian matrix.\n",
    "\n",
    "\\begin{gather*}\n",
    "V = Z \\\\\n",
    "W = \\frac{\\sqrt{U}}{\\sqrt{Z^TZ}}\n",
    "\\end{gather*}\n",
    "\n",
    "\\begin{gather*}\n",
    "X = VW \\\\\n",
    "Y = V^TV\n",
    "\\end{gather*}\n",
    "\n",
    "The determinant of Jacobian matrix is\n",
    "\n",
    "\\begin{align*}\n",
    "\\left|\\frac{\\partial(x, y)}{\\partial(z, u)}\\right| &= \\left|\\frac{\\partial(v, w)}{\\partial(z, u)}\\right|\\left|\\frac{\\partial(x, y)}{\\partial(v, w)}\\right| \\\\\n",
    "&=\n",
    "\\begin{vmatrix}\n",
    "I_p & 0 \\\\\n",
    "-\\sqrt{u}|z^Tz|^{-3/2}z & \\frac{1}{2\\sqrt{u}}\\frac{1}{\\sqrt{z^Tz}}\n",
    "\\end{vmatrix}\n",
    "\\times\n",
    "\\begin{vmatrix}\n",
    "wI_p & v \\\\\n",
    "2v^T & 0\n",
    "\\end{vmatrix} \\\\\n",
    "&= \\left|\\frac{1}{2\\sqrt{u}\\sqrt{z^Tz}}\\right| \\left| wI_p \\right| \\left| 0-2v^T(wI)^{-1}v \\right| \\\\\n",
    "&= \\frac{|w|^{p-1}v^Tv}{\\sqrt{u}\\sqrt{z^Tz}} = \\left( \\frac{u}{z^Tz}\\right)^{\\frac{p}{2} -1 }\n",
    "\\end{align*}\n",
    "\n",
    "Since $X \\sim N_p(\\mathbf{0}, \\mathbf{I})$ and $Y \\sim \\chi^2_p$ are independent, the joint distribution of $Z$ and $U$ is\n",
    "\n",
    "\\begin{align*}\n",
    "f_{ZU}(z, u) &= f_{XY}(x, y)\\left|\\frac{\\partial(x, y)}{\\partial(z, u)}\\right| \\\\\n",
    "&= \\left| 2\\pi I_p \\right|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}x^Tx\\right) \\frac{1}{\\Gamma\\left(\\frac{p}{2}\\right)2^{\\frac{p}{2}}}y^{\\frac{p}{2} - 1} \\exp \\left(-\\frac{y}{2}\\right)\\left(\\frac{u}{z^Tz}\\right)^{\\frac{p}{2}-1} \\\\\n",
    "&= \\left| 2\\pi I_p \\right|^{-\\frac{1}{2}}\\exp\\left(-\\frac{u}{2}\\right) \\frac{1}{\\Gamma\\left(\\frac{p}{2}\\right)2^{\\frac{p}{2}}}\\left(z^Tz\\right)^{\\frac{p}{2} - 1} \\exp \\left(-\\frac{z^Tz}{2}\\right)\\left(\\frac{u}{z^Tz}\\right)^{\\frac{p}{2}-1} \\\\\n",
    "&= \\left| 2\\pi I_p \\right|^{-\\frac{1}{2}}\\exp\\left(-\\frac{z^Tz}{2}\\right) \\frac{1}{\\Gamma\\left(\\frac{p}{2}\\right)2^{\\frac{p}{2}}}\\left(u\\right)^{\\frac{p}{2} - 1} \\exp \\left(-\\frac{u}{2}\\right)I\\left\\{u > 0\\right\\} \\\\\n",
    "&= f_Z(z)f_U(u)\n",
    "\\end{align*}\n",
    "\n",
    "Since the joint distribution is splited into two functions, $Z$ and $U$ are independent and by integrating with respect to $u$, we get\n",
    "\n",
    "$$\n",
    "f_Z(z) = \\left| 2\\pi I_p \\right|^{-\\frac{1}{2}}\\exp\\left(-\\frac{z^Tz}{2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The written code is attached with the homework file.(pass_value.c) The result of the code and its short explanation is as follows:\n",
    "\n",
    "*1. Integer variable **d** and its pointer **pd** are defined.*\n",
    "\n",
    "---Outside the function---\n",
    "\n",
    "address of the integer is &d = 0x7ffff2566d4c\n",
    "\n",
    "value of the pointer is pd = 0x7ffff2566d4c\n",
    "\n",
    "address of the pointer to the integer is &pd = 0x7ffff2566d50\n",
    "\n",
    "*2. Since **pd** points the address of **d**, address of the integer **d** is equal to the value of the pointer **pd**.*\n",
    "\n",
    "----Inside the function before increment----\n",
    "\n",
    "address of the integer is &i = 0x7ffff2566d2c\n",
    "\n",
    "value of the pointer is pi = 0x7ffff2566d4c\n",
    "\n",
    "address of the pointer to the integer is &pi = 0x7ffff2566d20\n",
    "\n",
    "*3. When a function is called, its arguments are copied and assigned to a memory. In this case, value of **d**, 0 is copied and assigned to new memory(**i**) and the value of **pd** is copied to the memory of **pi**. The value of the pointer **pi** is equal to that of **pd**, but their addresses are different.*\n",
    "\n",
    "----Inside the function after increment----\n",
    "\n",
    "address of the integer is &i = 0x7ffff2566d2c\n",
    "\n",
    "value of the pointer is pi = 0x7ffff2566d50\n",
    "\n",
    "address of the pointer to the integer is &pi = 0x7ffff2566d20\n",
    "\n",
    "*4. After increment, the address of **i** and **pi** does not change. However, the value of **i** are increased by 1 and the value of **pi** is increased by 4. Since integer type variable has 4 byte, adding 1 to the pointer results in 4 increament. Also, note in this case that value of the pointer **pi** is equal to the address of **pd**.*\n",
    "\n",
    "---Outside the function---\n",
    "\n",
    "address of the integer is &d = 0x7ffff2566d4c\n",
    "\n",
    "value of the pointer is pd = 0x7ffff2566d4c\n",
    "\n",
    "address of the pointer to the integer is &pd = 0x7ffff2566d50\n",
    "\n",
    "*5. Since the function passes its arguements by reference, the address of integer **&d**, value of the pointer **pd**, and the address of the pointer to the integer **&pd** all do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "We can write a function that executes the matrix multiplication algorith using CCS representation that I wrote in the proveious homework. The code is attached with this document.(ccs_product.c)\n",
    "\n",
    "Function *cssprod* takes *x* array, *y* array, *a* array, *r* array, *c* array, and the length of *a* array: *lena*. This algortithm does the same work as that in the homework2 does.\n",
    "\n",
    "In main function, an example for performing the matrix multiplication is given. In  this example, the matrix and array are given as follows:\n",
    "\n",
    "\\begin{gather*}\n",
    "W \\times x =\n",
    "\\begin{pmatrix}\n",
    "0  &  1  &  0  &  0  &  0  &  0  &  4 \\\\\n",
    "4  &  0  & 10  &  0  &  0  &  0  &  5 \\\\\n",
    "0  & 13 &  -4  &  0  &  0  &  0  &  0 \\\\\n",
    "2  &  0  &  0  &  0  &  3  &  0  &  0 \\\\\n",
    "0  &  0  &  0  &  0  &  0  &  0  &  0 \\\\\n",
    "0  &  0  &  0  &  0  &  0  &  0  &  0 \\\\\n",
    "\\end{pmatrix} \\times \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5.5 \\\\\n",
    "6 \\\\\n",
    "7 \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "30.0 \\\\\n",
    "69.0 \\\\\n",
    "14.0 \\\\\n",
    "18.5 \\\\\n",
    "0.0 \\\\\n",
    "0.0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "a = (4, 2, 1, 13, 10, -4, 3, 4, 5) \\\\\n",
    "r = (1, 3, 0, 2, 1, 2, 3, 0, 1) \\\\\n",
    "c = (0, 2, 4, 6, 6, 7, 7, 9) \\\\\n",
    "x = (1,2,3,4,5.5,6,7)\n",
    "\\end{gather*}\n",
    "\n",
    "In main function, *W* is not stored and *x*, *a*, *r*, *c* are only stored in memory. Also, *y* is initialized to 0 array.\n",
    "\n",
    "We can check the expected output in C as follows\n",
    "\n",
    "y[0] = 30.000000 y[1] = 69.000000 y[2] = 14.000000 y[3] = 18.500000 y[4] = 0.000000 y[5] = 0.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "## (a)\n",
    "\n",
    "To find the Maximum Likelihood Estimator for $\\mu$ and $\\Sigma$, first, assume that $p_1 \\geq p_2 \\geq \\cdots \\geq p_n$, without loss of generality, since we can swap the index of samples to satisfy this condition.\n",
    "\n",
    "Now write $n$ by $p$ matrix $X$\n",
    "\\begin{equation}\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "& * & * & * & \\cdots \\\\\n",
    "z_1 & & * & * & \\cdots \\\\\n",
    "& z_2 & z_3 & *  & \\cdots\\\\\n",
    "& & & z_4 & \\cdots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where $*$ refers to missing data. \n",
    "\n",
    "Also, denote $n_1 =$ length of $z_1, \\cdots, n_p=$ length of $z_p$\n",
    "\n",
    "The log pdf of $z_1 , \\cdots , z_p$ can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "\\log_{\\mu, \\Sigma} f(z_1, \\cdots, z_p) &=\\log_{\\mu, \\Sigma} f(z_1) + \\log_{\\mu, \\Sigma} (z_2 | z_1) + \\cdots \\log_{\\mu, \\Sigma}(z_p | z_1, \\cdots z_{p-1}) \\\\\n",
    "&=\\log_{\\mu, \\Sigma} f(z_1) + \\log_{\\mu, \\Sigma} (z_2 | z_{1(z_2)}) + \\cdots \\log_{\\mu, \\Sigma}(z_p | z_{1(z_p)}, \\cdots z_{p-1(z_p)}) \n",
    "\\end{align*}\n",
    "\n",
    "where $z_{k(z_j)}$ is a length of $n_j$ vector extracted from $z_k$. For instance, we can write\n",
    "\n",
    "\\begin{equation*}\n",
    "z_1 =\n",
    "\\begin{pmatrix}\n",
    "\\vdots\\\\\n",
    "z_{1(z_2)}\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Now assume that we have mle of $k-1$ vector $\\mu_{k-1}$ and $k-1 \\times k-1$ vector $\\Sigma_{(k-1)(k-1)}$  matrix. Then we try to find a scalar $\\mu_k$, $k-1$ vector $\\Sigma_{(k-1)k}$, and scalar $\\Sigma_{kk}$ inductively\n",
    "\n",
    "Note that for $k = 2, \\cdots, k = p$, the conditional distribution of $z_k$ can be written as\n",
    "$$\n",
    "z_k|\\mathcal{X}_{k} \\sim N_{n_k}(\\mathbb{1}_{n_k}\\beta_{k0} + \\mathcal{X}_{k}\\beta_{k1}, \\sigma_k^2 I_{n_k})\n",
    "$$\n",
    "\n",
    "where $\\mathcal{X}_{k}$ is $n_k \\times (k-1)$ matrix\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{X}_{k} = \n",
    "\\begin{pmatrix}\n",
    "z_{1(z_k)} & z_{2(z_k)} & \\cdots & z_{k-1(z_k)}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "This is because, writting $z_{j(z_k)} =(z_{j(z_k), 1}, \\cdots, z_{j(z_k), n_k})^T$, $z_{k-1, i}^* = (z_{1(z_k), i}, \\cdots, z_{k-1(z_k), i})^T$, for $i = 1, \\cdots n_k$,\n",
    "\n",
    "$$\n",
    "z_{ki} | z_{k-1, i}^* \\stackrel{i.i.d.}{\\sim} N_{n_k}\\left( \\mu_k + \\Sigma_{k(k-1)}\\Sigma_{(k-1)(k-1)}^{-1}(z_{k-1, i}^* - \\mu_{k-1}), \\sigma_{kk \\cdot (k-1)} \\right)\n",
    "$$\n",
    "\n",
    "where $\\mu_{k-1}$, $\\Sigma_{(k-1)k}$ and $\\Sigma_{(k-1)(k-1)}$ are $k-1$ vector, $k-1$ vector, and $(k-1) \\times (k-1)$ matrix each. This implies that\n",
    "\n",
    "\\begin{align*}\n",
    "z_k|\\mathcal{X}_{k} =\n",
    "\\begin{pmatrix}\n",
    "z_{k1} | z_{k-1, 1}^* \\\\\n",
    "\\vdots \\\\\n",
    "z_{k n_k} | z_{k-1, n_k}^*\n",
    "\\end{pmatrix}\n",
    "&\\sim\n",
    "N_{n_k}\\left( \\mathbb{1}_{n_k}\\mu_k + [\\mathcal{X}_{k} - \\mathbb{1}_{n_k}\\mu_{k-1}^T]\\Sigma_{(k-1)(k-1)}^{-1}\\Sigma_{(k-1)k}, \\ \\sigma_{kk\\cdot (k-1)} I_{n_k} \\right) \\\\\n",
    "&\\stackrel{let}{=} N_{n_k}(\\mathbb{1}_{n_k}\\beta_{k0} + \\mathcal{X}_{k}\\beta_{k1}, \\sigma_k^2 I_{n_k})\n",
    "\\end{align*}\n",
    "\n",
    "Due to invariance property of MLE, we can find the MLE of $\\mu, \\Sigma$ by obtaining the MLE of $\\beta_{k0}$'s, $\\beta_{k1}$'s, and $\\sigma_{k}^2$. Moreover, finding the MLE of such $\\beta_{k0}$'s, $\\beta_{k1}$'s, and $\\sigma_{k}^2$ is equivalent to finding the MLE of regression model:\n",
    "\n",
    "$$\n",
    "z_k = \\mathrm{1}_{n_k}\\beta_{k0} + \\mathcal{X}_{k}\\beta_{k1} + \\varepsilon\n",
    "$$\n",
    "\n",
    "where $\\varepsilon \\sim N_{n_k}(0, \\sigma_k^2 I_{n_k})$\n",
    "\n",
    "Recall that the MLE of regression model is\n",
    "\n",
    "\\begin{gather*}\n",
    "\\hat{\\beta}_{k1} = \\left[\\mathcal{X}_{k}^T\\left(I_{n_k}-\\frac{1}{n_k}\\mathbb{1}_{n_k}\\mathbb{1}_{n_k}^T\\right)\\mathcal{X}_k\\right]^{-1}\\mathcal{X}_k^T(z_k - \\mathbb{1}_{n_k}\\bar{z_k}) \\\\\n",
    "\\hat{\\beta}_{k0} = \\bar{z}_k - \\frac{1}{n_k}\\mathbb{1}_{n_k}^T\\mathcal{X}_k\\hat{\\beta}_{k1} \\\\\n",
    "\\hat{\\sigma}_k^2 = \\frac{1}{n_k}|| z_k - \\mathbb{1}_{n_k}\\hat{\\beta}_{k0} - \\mathcal{X}_k\\hat{\\beta}_{k1} ||^2\n",
    "\\end{gather*}\n",
    "\n",
    "Now, from the above parametrazation, it follows that\n",
    "\n",
    "\\begin{gather*}\n",
    "\\Sigma_{(k-1)k} = \\Sigma_{(k-1)(k-1)}\\hat{\\beta}_{k1} \\\\\n",
    "\\mu_k = \\mu_{k-1}^T\\hat{\\beta_{k1}} + \\hat{\\beta_{k0}} \\\\\n",
    "\\sigma_{kk}^2 = \\hat{\\beta}_{k1}^T\\Sigma_{(k-1)(k-1)}\\hat{\\beta}_{k1} + \\hat{\\sigma}_{k}^2\n",
    "\\end{gather*}\n",
    "\n",
    "Hence, we can find the MLE inductively, starting from\n",
    "\n",
    "\\begin{gather*}\n",
    "\\hat{\\mu}_1 = \\frac{1}{n}\\sum_i^n x_{i1}\\\\\n",
    "\\hat{\\sigma}_{11} = \\frac{1}{n}\\sum_i^n(x_{i1} - \\bar{x}_1)^2\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "Suppose that we observe all data and there is no missingness. The complete log-likelihood is\n",
    "\n",
    "$$\n",
    "l(\\mu, \\Sigma; X_1, \\cdots , X_n) = -\\frac{n}{2}\\log |2\\pi \\Sigma| - \\frac{1}{2} \\sum_i (X_i - \\mu)^T \\Sigma^{-1}(X_i - \\mu)\n",
    "$$\n",
    "\n",
    "E-step\n",
    "---\n",
    "Now, conditioning the log-likelihood on the observed data $X^o$,\n",
    "\n",
    "\\begin{align*}\n",
    "E[l(\\mu, \\Sigma; X_1, \\cdots , X_n)|X^o] &= -\\frac{n}{2}\\log |2\\pi \\Sigma| - \\frac{1}{2} \\sum_i tr\\left(\\Sigma^{-1}E\\left [(X_i - \\mu)(X_i - \\mu)^T | x_{i1}, \\cdots, x_{ip_i}\\right] \\right) \\\\\n",
    "&= -\\frac{n}{2}\\log |2\\pi \\Sigma| - \\frac{1}{2} \\sum_i tr\\left(\\Sigma^{-1}\\left [(X_iX_i^T)^* - X_i^*\\mu^T - \\mu X_i^{*T} + \\mu\\mu^T \\right] \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "E\\left[ X_i|x_{i1} , \\cdots, x_{ip_i} \\right] = E\\left[ X_i|X_i^o \\right] = \n",
    "\\begin{pmatrix}\n",
    "X_i^o & \\mu_{i2} + \\Sigma_{i21}\\Sigma_{i11}^{-1}(X_i^o - \\mu_{i1}) \n",
    "\\end{pmatrix} \\stackrel{let}{=} X_i^*\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "E\\left[ X_iX_i^T|X_i^o \\right] = \n",
    "\\begin{pmatrix}\n",
    "X_i^o & X_i^* \\\\\n",
    "X_i^{*T} & \\Sigma_{i22} - \\Sigma_{i21}\\Sigma_{i11}^{-1}\\Sigma_{i12} + X_i^*X_i^{*T}\n",
    "\\end{pmatrix} \\stackrel{let}{=} (X_iX_i^T)^*\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{i1} &= E[X_i^o] \\\\\n",
    "\\Sigma_{i11} &= Var[X_i^o] \\\\\n",
    "\\Sigma_{i21} &= Cov[X_i^M, X_i^o] \\\\\n",
    "\\Sigma_{i22} &= Var[X_i^M] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "M-step\n",
    "---\n",
    "\n",
    "Now, maximize \n",
    "\n",
    "\\begin{multline*}\n",
    "E[l(\\mu, \\Sigma; X_1, \\cdots , X_n)|X^o] = -\\frac{n}{2}\\log |2\\pi \\Sigma| \\\\ - \\frac{1}{2} \\sum_i tr\\left(\\Sigma^{-1}\\left [(X_iX_i^T)^* - X_i^*X_i^{*T} \\right] \\right)  - \\frac{1}{2} \\sum_i \\left((\\mu - X_i^*)^T\\Sigma^{-1}(\\mu-X_i^*)  \\right) \n",
    "\\end{multline*}\n",
    "\n",
    "Differentiating with respect to $\\mu$ and solving the estimating equation,\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}^{(k+1)} = \\frac{1}{n}\\sum_i^nX_i^*\n",
    "$$\n",
    "\n",
    "Then differentiating with respect tot $\\Sigma$ and solving the estimating equation, we get\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma}^{(k+1)} = \\frac{1}{n}\\left(\\sum_i (X_iX_i^T)^* - \\left(\\sum_iX_i^*\\right)\\left(\\sum_iX_i^*\\right)^T \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n",
    "The number of computations needed in finding MLE depends on $\\beta_{k1}$ and $\\beta_{k0}$. If we are regressing $n$ vector $y$ to $n \\times p$ matrix $X$ the computational complexity would be dominated by\n",
    "\n",
    "\\begin{gather*}\n",
    "X^TX \\implies p^2n\\\\\n",
    "X^Ty \\implies pn\\\\\n",
    "(X^TX)^{-1}X^Ty \\implies p^3\n",
    "\\end{gather*}\n",
    "\n",
    "Since $pn$ and$p^3$ can be regarded as negligible, the computation is determined by $p^2n$. In this problem, since we are regressing $n_k \\times k$ matrix $(\\mathbb{1}_{n_k} \\mathcal{X}_k)$ to $n_k$ vector $z_k$, the computational complexity is\n",
    "\n",
    "$$\n",
    "o\\left(\\sum_{k = 1}^p k^2n_k\\right)\n",
    "$$\n",
    "\n",
    "On the other hand, the number of computations needed in one EM step is dominated by\n",
    "\n",
    "$$\n",
    "\\Sigma_{i11}^{-1}\\Sigma_{i12} \\implies p_i^2q_i\n",
    "$$\n",
    "\n",
    "where $p_i$ is the number of observed variables for $i$-th data and $q_i$ is the numbrer of missing variables for $i$-th data.\n",
    "\n",
    "So the computational complexity for EM algorithm is\n",
    "\n",
    "$$\n",
    "o\\left(\\sum_{i=1}^n p_i^2q_i\\right)\n",
    "$$\n",
    "\n",
    "To compare the number of computations, we assume that $n_k = m(p-k+1)$, That is, for $k$-th column, the number of missing data is $m(k-1)$ each. Then the computational complexity for MLE is\n",
    "\n",
    "$$\n",
    "o\\left(\\sum_{k = 1}^p k^2n_k\\right) = o\\left(m\\sum_{k=1}^pk^2(p-k)\\right) = o\\left(\\frac{mp^4}{12}\\right)\n",
    "$$\n",
    "\n",
    "For EM algorithm,\n",
    "\n",
    "$$\n",
    "o\\left(\\sum_{i=1}^n p_i^2q_i\\right) = o\\left(m\\sum_{k=1}^pk(p-k)\\right) = o\\left(\\frac{mp^3}{6}\\right)\n",
    "$$\n",
    "\n",
    "If p is large enough, one EM step would be efficient than direct maximization of the loglikelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)\n",
    "\n",
    "Suppose $p_i$ observed coordinates are not the first ones. Then it is possible that we cannot apply direct maximiazation of loglikelihood as we did in (a). Since the first column still has mising data, we cannot find mle inductively from $\\mu_1$ and $\\Sigma_11$. Thus, we have to find the maximum likelihood by iterative method.\n",
    "\n",
    "However, for EM algorithm, by swapping the index of columns appropriately, we can use the same method as above. So the number of computational would be proportional to $o\\left(\\frac{mp^3}{6}\\right)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
